{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and set constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "INPUT_PATH = \"sdc2023\"\n",
    "\n",
    "WGS84_SEMI_MAJOR_AXIS = 6378137.0\n",
    "WGS84_SEMI_MINOR_AXIS = 6356752.314245\n",
    "WGS84_SQUARED_FIRST_ECCENTRICITY  = 6.69437999013e-3\n",
    "WGS84_SQUARED_SECOND_ECCENTRICITY = 6.73949674226e-3\n",
    "\n",
    "HAVERSINE_RADIUS = 6_371_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class to manage converting between BLH and GNSS\n",
    "Needed since ground truth is in BLH (latitude, longitude, height) and GNSS is in ECEF (x, y z with COM as origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ECEF:\n",
    "    x: np.ndarray\n",
    "    y: np.ndarray\n",
    "    z: np.ndarray\n",
    "\n",
    "    def to_numpy(self):\n",
    "        return np.stack([self.x, self.y, self.z], axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_numpy(pos: np.ndarray):\n",
    "        x, y, z = [np.squeeze(w) for w in np.split(pos, 3, axis=-1)]\n",
    "        return ECEF(x=x, y=y, z=z)\n",
    "\n",
    "@dataclass\n",
    "class BLH:\n",
    "    lat: np.ndarray\n",
    "    lng: np.ndarray\n",
    "    hgt: np.ndarray = 0\n",
    "        \n",
    "\n",
    "def ECEF_to_BLH(ecef: ECEF) -> BLH:\n",
    "    \"\"\"\n",
    "    Convert Earth-Centered, Earth-Fixed (ECEF) coordinates to geodetic coordinates (latitude, longitude, height).\n",
    "\n",
    "    Args:\n",
    "        ecef (ECEF): ECEF coordinates (x, y, z).\n",
    "\n",
    "    Returns:\n",
    "        BLH: Geodetic coordinates (latitude, longitude, height).\n",
    "    \"\"\"\n",
    "    a = WGS84_SEMI_MAJOR_AXIS\n",
    "    b = WGS84_SEMI_MINOR_AXIS\n",
    "    e2  = WGS84_SQUARED_FIRST_ECCENTRICITY\n",
    "    e2_ = WGS84_SQUARED_SECOND_ECCENTRICITY\n",
    "    x = ecef.x\n",
    "    y = ecef.y\n",
    "    z = ecef.z\n",
    "    r = np.sqrt(x**2 + y**2)\n",
    "    t = np.arctan2(z * (a/b), r)\n",
    "    B = np.arctan2(z + (e2_*b)*np.sin(t)**3, r - (e2*a)*np.cos(t)**3)\n",
    "    L = np.arctan2(y, x)\n",
    "    n = a / np.sqrt(1 - e2*np.sin(B)**2)\n",
    "    H = (r / np.cos(B)) - n\n",
    "    # convert to degrees\n",
    "    B = np.rad2deg(B)\n",
    "    L = np.rad2deg(L)\n",
    "    return BLH(lat=B, lng=L, hgt=H)\n",
    "\n",
    "def haversine_distance(blh_1: BLH, blh_2: BLH) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the haversine distance between two sets of points on the Earth's surface.\n",
    "\n",
    "    Args:\n",
    "        blh_1 (BLH): Geodetic coordinates of the first point.\n",
    "        blh_2 (BLH): Geodetic coordinates of the second point.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Haversine distance between the two sets of points.\n",
    "    \"\"\"\n",
    "    lat_1 = np.deg2rad(blh_1.lat)\n",
    "    lng_1 = np.deg2rad(blh_1.lng)\n",
    "    lat_2 = np.deg2rad(blh_2.lat)\n",
    "    lng_2 = np.deg2rad(blh_2.lng)\n",
    "    \n",
    "    dlat = lat_2 - lat_1\n",
    "    dlng = lng_2 - lng_1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat_1) * np.cos(lat_2) * np.sin(dlng/2)**2\n",
    "    dist = 2 * HAVERSINE_RADIUS * np.arcsin(np.sqrt(a))\n",
    "    return dist\n",
    "\n",
    "def pandas_haversine_distance(df1: pd.DataFrame, df2: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate the haversine distance between two sets of geodetic coordinates.\n",
    "\n",
    "    Args:\n",
    "        df1 (pd.DataFrame): First set of geodetic coordinates.\n",
    "        df2 (pd.DataFrame): Second set of geodetic coordinates.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Haversine distance between the two sets of coordinates.\n",
    "    \"\"\"\n",
    "    blh1 = BLH(\n",
    "        lat=df1['LatitudeDegrees'].to_numpy(),\n",
    "        lng=df1['LongitudeDegrees'].to_numpy(),\n",
    "        hgt=0,\n",
    "    )\n",
    "    blh2 = BLH(\n",
    "        lat=df2['LatitudeDegrees'].to_numpy(),\n",
    "        lng=df2['LongitudeDegrees'].to_numpy(),\n",
    "        hgt=0,\n",
    "    )\n",
    "    return haversine_distance(blh1, blh2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecef_to_lat_lng(tripID: str, gnss_df: pd.DataFrame, UnixTimeMillis: pd.Series | np.ndarray) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert ECEF coordinates to geodetic coordinates (latitude, longitude).\n",
    "\n",
    "    Args:\n",
    "        tripID (str): Trip ID.\n",
    "        gnss_df (pd.DataFrame): GNSS data.\n",
    "        UnixTimeMillis (pd.Series | np.ndarray): Unix time in milliseconds.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Geodetic coordinates (latitude, longitude).\n",
    "    \"\"\"\n",
    "    ecef_columns = ['WlsPositionXEcefMeters', 'WlsPositionYEcefMeters', 'WlsPositionZEcefMeters']\n",
    "    columns = ['utcTimeMillis'] + ecef_columns\n",
    "    ecef_df = (gnss_df.drop_duplicates(subset='utcTimeMillis')[columns]\n",
    "               .dropna().reset_index(drop=True))\n",
    "    ecef = ECEF.from_numpy(ecef_df[ecef_columns].to_numpy())\n",
    "    blh  = ECEF_to_BLH(ecef)\n",
    "\n",
    "    TIME = ecef_df['utcTimeMillis'].to_numpy()\n",
    "    lat = InterpolatedUnivariateSpline(TIME, blh.lat, ext=3)(UnixTimeMillis)\n",
    "    lng = InterpolatedUnivariateSpline(TIME, blh.lng, ext=3)(UnixTimeMillis)\n",
    "    return pd.DataFrame({\n",
    "#         'tripId' : tripID,\n",
    "        'utcTimeMillis'   : UnixTimeMillis,\n",
    "        'LatitudeDegrees'  : np.degrees(lat),\n",
    "        'LongitudeDegrees' : np.degrees(lng),\n",
    "    })\n",
    "\n",
    "def calc_score(pred_df: pd.DataFrame, gt_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the score of the predicted trajectory.\n",
    "\n",
    "    Args:\n",
    "        pred_df (pd.DataFrame): Predicted trajectory.\n",
    "        gt_df (pd.DataFrame): Ground truth trajectory.\n",
    "\n",
    "    Returns:\n",
    "        float: Score of the predicted trajectory.\n",
    "    \"\"\"\n",
    "    d = pandas_haversine_distance(pred_df, gt_df)\n",
    "    score = np.mean([np.quantile(d, 0.50), np.quantile(d, 0.95)])    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_comparison(lat, lng, gt_lat, gt_lng):\n",
    "    for lat_val, lng_val, gt_lat_val, gt_lng_val in zip(lat, lng, gt_lat, gt_lng):\n",
    "        print(f'Pred: ({lat_val:<12.7f}, {lng_val:<12.7f}) Ground Truth: ({gt_lat_val:<12.7f}, {gt_lng_val:<12.7f})')\n",
    "        \n",
    "def print_batch(amnt: int, lat_arr: np.ndarray, lng_arr: np.ndarray, gt_lat_arr: np.ndarray, gt_lng_arr: np.ndarray):\n",
    "    for batch in range(amnt):\n",
    "        print(f'Val data {batch}')\n",
    "        print_comparison(lat_arr[batch], lng_arr[batch], gt_lat_arr[batch], gt_lng_arr[batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-25-00-34-us-ca-mtv-sb-101/pixel4\n",
      "2020-06-25-00-34-us-ca-mtv-sb-101/pixel4xl\n",
      "2020-07-08-22-28-us-ca/pixel4\n",
      "2020-07-08-22-28-us-ca/pixel4xl\n",
      "2020-07-17-22-27-us-ca-mtv-sf-280/pixel4\n",
      "2020-07-17-23-13-us-ca-sf-mtv-280/pixel4\n",
      "2020-07-17-23-13-us-ca-sf-mtv-280/pixel4xl\n",
      "2020-08-04-00-19-us-ca-sb-mtv-101/pixel4\n",
      "2020-08-04-00-20-us-ca-sb-mtv-101/pixel4xl\n",
      "2020-08-04-00-20-us-ca-sb-mtv-101/pixel5\n",
      "2020-08-13-21-41-us-ca-mtv-sf-280/pixel4\n",
      "2020-08-13-21-41-us-ca-mtv-sf-280/pixel4xl\n",
      "2020-08-13-21-42-us-ca-mtv-sf-280/pixel5\n",
      "2020-12-10-22-17-us-ca-sjc-c/mi8\n",
      "2020-12-10-22-52-us-ca-sjc-c/mi8\n",
      "2020-12-10-22-52-us-ca-sjc-c/pixel4\n",
      "2020-12-10-22-52-us-ca-sjc-c/pixel4xl\n",
      "2020-12-10-22-52-us-ca-sjc-c/pixel5\n",
      "2021-01-04-21-50-us-ca-e1highway280driveroutea/mi8\n",
      "2021-01-04-21-50-us-ca-e1highway280driveroutea/pixel4\n",
      "2021-01-04-21-50-us-ca-e1highway280driveroutea/pixel5\n",
      "2021-01-04-22-40-us-ca-mtv-a/mi8\n",
      "2021-01-04-22-40-us-ca-mtv-a/pixel4\n",
      "2021-01-04-22-40-us-ca-mtv-a/pixel5\n",
      "2021-01-05-21-12-us-ca-mtv-d/mi8\n",
      "2021-01-05-21-12-us-ca-mtv-d/pixel4\n",
      "2021-01-05-21-52-us-ca-mtv-d/mi8\n",
      "2021-01-05-21-52-us-ca-mtv-d/pixel4\n",
      "2021-03-10-23-13-us-ca-mtv-h/mi8\n",
      "2021-03-10-23-13-us-ca-mtv-h/pixel5\n",
      "2021-03-16-18-59-us-ca-mtv-a/mi8\n",
      "2021-03-16-18-59-us-ca-mtv-a/pixel5\n",
      "2021-03-16-18-59-us-ca-mtv-a/sm-g988b\n",
      "2021-03-16-19-00-us-ca-mtv-a/pixel4xl\n",
      "2021-03-16-20-40-us-ca-mtv-b/mi8\n",
      "2021-03-16-20-40-us-ca-mtv-b/pixel4xl\n",
      "2021-03-16-20-40-us-ca-mtv-b/pixel5\n",
      "2021-04-02-20-43-us-ca-mtv-f/mi8\n",
      "2021-04-02-20-43-us-ca-mtv-f/pixel5\n",
      "2021-04-02-20-43-us-ca-mtv-f/sm-g988b\n",
      "2021-04-08-21-28-us-ca-mtv-k/pixel5\n",
      "2021-04-08-21-28-us-ca-mtv-k/sm-g988b\n",
      "2021-04-26-23-38-us-ca-mtv-h/mi8\n",
      "2021-04-26-23-38-us-ca-mtv-h/sm-g988b\n",
      "2021-07-14-20-50-us-ca-mtv-e/pixel4\n",
      "2021-07-14-20-50-us-ca-mtv-e/pixel5\n",
      "2021-07-14-20-50-us-ca-mtv-e/sm-g988b\n",
      "2021-07-19-20-49-us-ca-mtv-a/mi8\n",
      "2021-07-19-20-49-us-ca-mtv-a/pixel4\n",
      "2021-07-19-20-49-us-ca-mtv-a/pixel5\n",
      "2021-07-19-20-49-us-ca-mtv-a/sm-g988b\n",
      "2021-07-27-19-49-us-ca-mtv-b/mi8\n",
      "2021-07-27-19-49-us-ca-mtv-b/pixel4\n",
      "2021-07-27-19-49-us-ca-mtv-b/pixel5\n",
      "2021-08-04-20-40-us-ca-sjc-c/sm-g988b\n",
      "2021-08-24-20-32-us-ca-mtv-h/mi8\n",
      "2021-08-24-20-32-us-ca-mtv-h/pixel4\n",
      "2021-08-24-20-32-us-ca-mtv-h/pixel5\n",
      "2021-08-24-20-32-us-ca-mtv-h/sm-g988b\n",
      "2021-12-07-19-22-us-ca-lax-d/mi8\n",
      "2021-12-07-19-22-us-ca-lax-d/pixel5\n",
      "2021-12-07-19-22-us-ca-lax-d/pixel6pro\n",
      "2021-12-07-19-22-us-ca-lax-d/sm-g988b\n",
      "2021-12-07-22-21-us-ca-lax-g/mi8\n",
      "2021-12-07-22-21-us-ca-lax-g/pixel5\n",
      "2021-12-07-22-21-us-ca-lax-g/pixel6pro\n",
      "2021-12-07-22-21-us-ca-lax-g/sm-g988b\n",
      "2021-12-08-17-22-us-ca-lax-a/pixel5\n",
      "2021-12-08-17-22-us-ca-lax-a/pixel6pro\n",
      "2021-12-08-18-52-us-ca-lax-b/mi8\n",
      "2021-12-08-18-52-us-ca-lax-b/pixel6pro\n",
      "2021-12-08-20-28-us-ca-lax-c/pixel5\n",
      "2021-12-08-20-28-us-ca-lax-c/pixel6pro\n",
      "2021-12-08-20-28-us-ca-lax-c/sm-g988b\n",
      "2021-12-09-17-06-us-ca-lax-e/pixel5\n",
      "2021-12-09-17-06-us-ca-lax-e/pixel6pro\n",
      "2022-01-11-18-48-us-ca-mtv-n/mi8\n",
      "2022-01-11-18-48-us-ca-mtv-n/pixel5\n",
      "2022-01-11-18-48-us-ca-mtv-n/pixel6pro\n",
      "2022-01-26-20-02-us-ca-mtv-pe1/mi8\n",
      "2022-01-26-20-02-us-ca-mtv-pe1/pixel5\n",
      "2022-01-26-20-02-us-ca-mtv-pe1/sm-g988b\n",
      "2022-02-24-18-29-us-ca-lax-o/mi8\n",
      "2022-02-24-18-29-us-ca-lax-o/pixel5\n",
      "2022-02-24-18-29-us-ca-lax-o/pixel6pro\n",
      "2022-04-01-18-22-us-ca-lax-t/mi8\n",
      "2022-04-01-18-22-us-ca-lax-t/pixel5\n",
      "2022-04-01-18-22-us-ca-lax-t/pixel6pro\n",
      "2022-05-13-20-57-us-ca-mtv-pe1/pixel6pro\n",
      "2022-05-13-20-57-us-ca-mtv-pe1/samsungs21ultra\n",
      "2022-05-13-20-57-us-ca-mtv-pe1/sm-g988b\n",
      "2022-07-26-21-01-us-ca-sjc-s/pixel5\n",
      "2022-07-26-21-01-us-ca-sjc-s/samsunga325g\n",
      "2022-08-04-20-07-us-ca-sjc-q/mi8\n",
      "2022-08-04-20-07-us-ca-sjc-q/pixel5\n",
      "2022-08-04-20-07-us-ca-sjc-q/sm-a325f\n",
      "2022-10-06-21-51-us-ca-mtv-n/sm-a205u\n",
      "2022-10-06-21-51-us-ca-mtv-n/sm-a217m\n",
      "2022-10-06-21-51-us-ca-mtv-n/sm-a325f\n",
      "2022-11-15-00-53-us-ca-mtv-a/pixel5\n",
      "2022-11-15-00-53-us-ca-mtv-a/pixel7\n",
      "2022-11-15-00-53-us-ca-mtv-a/pixel7pro\n",
      "2023-03-08-21-34-us-ca-mtv-u/pixel5\n",
      "2023-03-08-21-34-us-ca-mtv-u/pixel6pro\n",
      "2023-03-08-21-34-us-ca-mtv-u/pixel7pro\n",
      "2023-05-09-21-32-us-ca-mtv-pe1/pixel5\n",
      "2023-05-09-21-32-us-ca-mtv-pe1/pixel7pro\n",
      "2023-05-09-21-32-us-ca-mtv-pe1/samsungs22ultra\n",
      "2023-05-16-19-54-us-ca-mtv-xe1/pixel5\n",
      "2023-05-16-19-55-us-ca-mtv-xe1/pixel7pro\n",
      "2023-05-19-20-10-us-ca-mtv-ie2/pixel5\n",
      "2023-05-19-20-10-us-ca-mtv-ie2/pixel7pro\n",
      "2023-05-19-20-10-us-ca-mtv-ie2/sm-s908b\n",
      "2023-05-23-19-16-us-ca-mtv-ie2/pixel5\n",
      "2023-05-23-19-16-us-ca-mtv-ie2/sm-s908b\n",
      "2023-05-23-19-56-us-ca-mtv-ie2/sm-a226b\n",
      "2023-05-23-19-56-us-ca-mtv-ie2/sm-a505g\n",
      "2023-05-23-19-56-us-ca-mtv-ie2/sm-a600t\n",
      "2023-05-24-20-26-us-ca-sjc-ge2/pixel5\n",
      "2023-05-24-20-26-us-ca-sjc-ge2/pixel7pro\n",
      "2023-05-25-19-10-us-ca-sjc-be2/pixel5\n",
      "2023-05-25-19-10-us-ca-sjc-be2/pixel7pro\n",
      "2023-05-25-19-10-us-ca-sjc-be2/sm-s908b\n",
      "2023-05-25-20-11-us-ca-sjc-he2/pixel5\n",
      "2023-05-25-20-11-us-ca-sjc-he2/pixel7pro\n",
      "2023-05-25-20-11-us-ca-sjc-he2/sm-s908b\n",
      "2023-05-26-18-50-us-ca-sjc-ge2/sm-s908b\n",
      "2023-05-26-18-51-us-ca-sjc-ge2/pixel5\n",
      "2023-05-26-18-51-us-ca-sjc-ge2/pixel7pro\n",
      "2023-06-06-23-26-us-ca-sjc-he2/sm-a226b\n",
      "2023-09-05-20-13-us-ca/pixel5\n",
      "2023-09-05-20-13-us-ca/pixel7pro\n",
      "2023-09-05-20-59-us-ca/pixel4xl\n",
      "2023-09-05-20-59-us-ca/pixel5a\n",
      "2023-09-05-23-07-us-ca-routen/pixel5\n",
      "2023-09-05-23-07-us-ca-routen/pixel7pro\n",
      "2023-09-06-00-01-us-ca-routen/pixel4xl\n",
      "2023-09-06-00-01-us-ca-routen/pixel5a\n",
      "2023-09-06-00-01-us-ca-routen/pixel6pro\n",
      "2023-09-06-00-01-us-ca-routen/sm-g955f\n",
      "2023-09-06-18-04-us-ca/pixel5\n",
      "2023-09-06-18-04-us-ca/pixel7pro\n",
      "2023-09-06-18-04-us-ca/sm-s908b\n",
      "2023-09-06-18-47-us-ca/pixel4xl\n",
      "2023-09-06-18-47-us-ca/pixel6pro\n",
      "2023-09-06-22-49-us-ca-routebb1/pixel7pro\n",
      "2023-09-06-22-49-us-ca-routebb1/sm-s908b\n",
      "2023-09-07-18-59-us-ca/pixel5\n",
      "2023-09-07-18-59-us-ca/pixel7pro\n",
      "2023-09-07-19-33-us-ca/pixel4xl\n",
      "2023-09-07-19-33-us-ca/pixel5a\n",
      "2023-09-07-19-33-us-ca/pixel6pro\n",
      "2023-09-07-19-33-us-ca/sm-g955f\n",
      "2023-09-07-22-47-us-ca-routebc2/pixel6pro\n",
      "2023-09-07-22-48-us-ca-routebc2/pixel4xl\n",
      "2023-09-07-22-48-us-ca-routebc2/pixel5a\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "pred_dfs  = []\n",
    "gt_dfs = []\n",
    "\n",
    "\n",
    "\n",
    "for dirname in sorted(glob.glob(f'{INPUT_PATH}/train/*/*')):\n",
    "    drive, phone = dirname.split('/')[-2:]\n",
    "    tripID  = f'{drive}/{phone}'\n",
    "    gnss_df = pd.read_csv(f'{dirname}/device_gnss.csv')\n",
    "    gt_df   = pd.read_csv(f'{dirname}/ground_truth.csv')\n",
    "    \n",
    "    info_cols = ['IonosphericDelayMeters', 'TroposphericDelayMeters']\n",
    "    columns = ['utcTimeMillis'] + info_cols\n",
    "    info_df = (gnss_df.drop_duplicates(subset='utcTimeMillis')[columns].fillna(0).reset_index(drop=True))\n",
    "    \n",
    "    for col in info_cols:\n",
    "        info_df[col] = info_df[col].fillna((info_df[col].bfill() + info_df[col].ffill()) / 2)\n",
    "        \n",
    "    pred_df = ecef_to_lat_lng(tripID, gnss_df, gt_df['UnixTimeMillis'])\n",
    "    pred_df = pd.merge(pred_df, info_df, on='utcTimeMillis', how='left')\n",
    "    gt_df   = gt_df[['LatitudeDegrees', 'LongitudeDegrees']]\n",
    "    print(tripID)\n",
    "#     print(pred_df.shape)\n",
    "#     print(gt_df.shape)\n",
    "    pred_dfs.append(pred_df)\n",
    "    gt_dfs.append(gt_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Baseline Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This module injects some information about the relative or absolute position of the tokens in the sequence.\n",
    "    The positional encodings have the same dimension as the embeddings, so that the two can be summed. Here, we use\n",
    "    sine and cosine functions of different frequencies.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): The dimension of the model (i.e., the size of the input embeddings).\n",
    "        dropout (float): Dropout rate to apply after adding the positional encodings.\n",
    "        max_len (int): The maximum length of the input sequences for which to precompute positional encodings.\n",
    "\n",
    "    Attributes:\n",
    "        dropout (torch.nn.Dropout): Dropout layer to apply after adding positional encodings.\n",
    "        pe (torch.Tensor): Precomputed positional encodings.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimension of the model (i.e., the size of the input embeddings).\n",
    "            dropout (float): Dropout rate to apply after adding the positional encodings.\n",
    "            max_len (int): The maximum length of the input sequences for which to precompute positional encodings.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional encodings to the input tensor and applies dropout.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (seq_len, batch_size, embedding_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with positional encodings added, followed by dropout.\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# time sequence encoder\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder, which consists of an input linear layer to upscale the input \n",
    "    dimension to the model dimension, a positional encoding layer, a stack of Transformer encoder layers, and \n",
    "    a final fully connected (fc) layer for output transformation.\n",
    "\n",
    "    Args:\n",
    "        config (object): A configuration object containing the hyperparameters.\n",
    "    \n",
    "    Attributes:\n",
    "        config (object): The configuration object.\n",
    "        upscale (torch.nn.Linear): Linear layer to upscale input dimension to model dimension.\n",
    "        pos_encoder (PositionalEncoding): Positional encoding layer.\n",
    "        transformer (torch.nn.TransformerEncoder): Stack of Transformer encoder layers.\n",
    "        fc (torch.nn.Sequential): Fully connected layers for output transformation.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.upscale = torch.nn.Linear(config.input_dim, config.d_model) \n",
    "        self.pos_encoder = PositionalEncoding(config.d_model, config.dropout, config.max_seq_len)\n",
    "        self.transformer = torch.nn.TransformerEncoder(\n",
    "            torch.nn.TransformerEncoderLayer(\n",
    "                d_model=config.d_model,\n",
    "                nhead=config.nhead,\n",
    "                dim_feedforward=config.dim_feedforward,\n",
    "                dropout=config.dropout,\n",
    "                activation=config.activation,\n",
    "                batch_first=True\n",
    "            ),\n",
    "            num_layers=config.num_layers\n",
    "        )\n",
    "        self.fc = torch.nn.Sequential()\n",
    "        for i, num_neurons in enumerate(config.fc_layers[:-1]):\n",
    "            self.fc.add_module(\n",
    "                f'fc_{i}',\n",
    "                torch.nn.Linear(num_neurons, config.fc_layers[i+1])\n",
    "            )\n",
    "            if i < len(config.fc_layers) - 1:\n",
    "                self.fc.add_module(\n",
    "                    f'relu_{i}',\n",
    "                    torch.nn.ReLU()\n",
    "                )\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.upscale(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define baseline LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM Encoder, which consists of an input linear layer to upscale the input \n",
    "    dimension to the model dimension, a stack of LSTM layers, and a final fully connected (fc) layer for output transformation.\n",
    "\n",
    "    Args:\n",
    "        config (object): A configuration object containing the hyperparameters.\n",
    "    \n",
    "    Attributes:\n",
    "        config (object): The configuration object.\n",
    "        upscale (torch.nn.Linear): Linear layer to upscale input dimension to model dimension.\n",
    "        lstm (torch.nn.LSTM): Stack of LSTM layers.\n",
    "        fc (torch.nn.Sequential): Fully connected layers for output transformation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"\n",
    "        Initializes the LSTMEncoder module.\n",
    "\n",
    "        Args:\n",
    "            config (object): A configuration object containing the hyperparameters for the LSTM Encoder.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Linear layer to upscale the input dimension to the model dimension\n",
    "        self.upscale = torch.nn.Linear(config.input_dim, config.d_model)\n",
    "        \n",
    "        # Stack of LSTM layers\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=config.d_model,\n",
    "            hidden_size=config.d_model,\n",
    "            num_layers=config.num_layers,\n",
    "            dropout=config.dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for output transformation\n",
    "        self.fc = torch.nn.Sequential()\n",
    "        for i, num_neurons in enumerate(config.fc_layers[:-1]):\n",
    "            self.fc.add_module(\n",
    "                f'fc_{i}',\n",
    "                torch.nn.Linear(num_neurons, config.fc_layers[i+1])\n",
    "            )\n",
    "            if i < len(config.fc_layers) - 1:\n",
    "                self.fc.add_module(\n",
    "                    f'relu_{i}',\n",
    "                    torch.nn.ReLU()\n",
    "                )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the LSTMEncoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, input_dim).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after passing through the LSTM Encoder.\n",
    "        \"\"\"\n",
    "        # Upscale the input dimension to the model dimension\n",
    "        x = self.upscale(x)\n",
    "        \n",
    "        # Pass through the stack of LSTM layers\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # Transform the output through fully connected layers\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"\n",
    "    Configuration class to hold the hyperparameters and other settings.\n",
    "\n",
    "    Args:\n",
    "        config_dict (dict): A dictionary containing the hyperparameters and other settings.\n",
    "    \n",
    "    Attributes:\n",
    "        input_dim (int): The dimension of the input features.\n",
    "        d_model (int): The dimension of the model (i.e., the size of the input embeddings).\n",
    "        nhead (int): The number of heads in the multiheadattention models.\n",
    "        dim_feedforward (int): The dimension of the feedforward network model.\n",
    "        dropout (float): The dropout value.\n",
    "        activation (str): The activation function of intermediate layer, relu or gelu.\n",
    "        num_layers (int): The number of sub-encoder-layers in the encoder.\n",
    "        fc_layers (list[int]): The number of neurons in the fully connected layers.\n",
    "        output_dim (int): The dimension of the output features.\n",
    "        max_seq_len (int): The maximum sequence length.\n",
    "        val_split (float): The validation split ratio.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_dict):\n",
    "        self.input_dim = config_dict.get('input_dim')\n",
    "        self.d_model = config_dict.get('d_model')\n",
    "        self.nhead = config_dict.get('nhead')\n",
    "        self.dim_feedforward = config_dict.get('dim_feedforward')\n",
    "        self.dropout = config_dict.get('dropout')\n",
    "        self.activation = config_dict.get('activation')\n",
    "        self.num_layers = config_dict.get('num_layers')\n",
    "        self.fc_layers = config_dict.get('fc_layers')\n",
    "        self.output_dim = config_dict.get('output_dim')\n",
    "        self.max_seq_len = config_dict.get('max_seq_len')\n",
    "        self.val_split = config_dict.get('val_split')\n",
    "\n",
    "config_dict = {\n",
    "    \"input_dim\": 4, \n",
    "    \"d_model\": 8,\n",
    "    \"nhead\": 4,\n",
    "    \"dim_feedforward\": 128,\n",
    "    \"dropout\": 0.1,\n",
    "    \"activation\": \"relu\",\n",
    "    \"num_layers\": 6,\n",
    "    \"output_dim\": 2, \n",
    "    \"max_seq_len\": 3500,\n",
    "    \"val_split\": 0.05,\n",
    "}\n",
    "# For fc layers divide the d_model by 2 until it reaches the output_dim\n",
    "fc_layers = [config_dict[\"d_model\"]]\n",
    "while fc_layers[-1] > config_dict[\"output_dim\"]:\n",
    "    fc_layers.append(fc_layers[-1] // 2)\n",
    "\n",
    "fc_layers[-1] = config_dict[\"output_dim\"]\n",
    "config_dict[\"fc_layers\"] = fc_layers\n",
    "\n",
    "config = Config(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (upscale): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (pos_encoder): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=8, out_features=128, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=128, out_features=8, bias=True)\n",
       "        (norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (fc_0): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (relu_0): ReLU()\n",
       "    (fc_1): Linear(in_features=4, out_features=2, bias=True)\n",
       "    (relu_1): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TransformerEncoder(config)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNSSDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    This class represents a custom dataset for Global Navigation Satellite System (GNSS) data.\n",
    "    It processes prediction and ground truth dataframes, pads sequences to a maximum length,\n",
    "    and computes the residuals between predictions and ground truth positions.\n",
    "    Args:\n",
    "        pred_dfs (list of pandas.DataFrame): List of dataframes containing prediction data.\n",
    "        gt_dfs (list of pandas.DataFrame): List of dataframes containing ground truth data.\n",
    "    \n",
    "    Attributes:\n",
    "        pred_dfs (list of pandas.DataFrame): List of dataframes containing prediction data.\n",
    "        gt_dfs (list of pandas.DataFrame): List of dataframes containing ground truth data.\n",
    "        sequences (numpy.ndarray): Numpy array of padded prediction sequences.\n",
    "        labels (numpy.ndarray): Numpy array of residuals (ground truth - prediction).\n",
    "    \"\"\"\n",
    "    def __init__(self, pred_dfs, gt_dfs):\n",
    "        \"\"\"\n",
    "        Initializes the GNSSDataset.\n",
    "        Args:\n",
    "            pred_dfs (list of pandas.DataFrame): List of dataframes containing prediction data.\n",
    "            gt_dfs (list of pandas.DataFrame): List of dataframes containing ground truth data.\n",
    "        \"\"\"\n",
    "        self.pred_dfs = pred_dfs\n",
    "        self.gt_dfs = gt_dfs\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for pred_df in self.pred_dfs:\n",
    "            x_np = pred_df[['LatitudeDegrees', 'LongitudeDegrees', 'IonosphericDelayMeters', 'TroposphericDelayMeters']].to_numpy()\n",
    "            ## pad to max sequence length\n",
    "            pad = np.zeros((config.max_seq_len - x_np.shape[0], x_np.shape[1]))\n",
    "            x_np = np.vstack([x_np, pad])\n",
    "            #x_np = x_np/180\n",
    "            self.sequences.append(x_np)\n",
    "            \n",
    "        for gt_df in self.gt_dfs:\n",
    "            y_np = gt_df[['LatitudeDegrees', 'LongitudeDegrees']].to_numpy()\n",
    "            ## pad to max sequence length\n",
    "            pad = np.zeros((config.max_seq_len - y_np.shape[0], y_np.shape[1]))\n",
    "            y_np = np.vstack([y_np, pad])\n",
    "            #y_np = y_np/180\n",
    "            self.labels.append(y_np)\n",
    "\n",
    "        self.sequences = np.array(self.sequences, dtype=np.float32)\n",
    "        self.labels = np.array(self.labels,  dtype=np.float32)\n",
    "        \n",
    "        self.labels = self.labels - self.sequences[:, :, :2] # just the residuals\n",
    "        \n",
    "        print(\"seq and label shapes\")\n",
    "        print(self.sequences.shape)\n",
    "        print(self.labels.shape)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        Retrieves the sequence and label at index i.\n",
    "        Args:\n",
    "            i (int): Index of the data to retrieve.\n",
    "        Returns:\n",
    "            tuple: (sequence, label) at index i.\n",
    "        \"\"\"\n",
    "        return self.sequences[i], self.labels[i]\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of sequences in the dataset.\n",
    "        Returns:\n",
    "            int: Number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return self.sequences.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.800000000000001\n"
     ]
    }
   ],
   "source": [
    "print(len(pred_dfs)*config.val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq and label shapes\n",
      "(148, 3500, 4)\n",
      "(148, 3500, 2)\n",
      "seq and label shapes\n",
      "(8, 3500, 4)\n",
      "(8, 3500, 2)\n"
     ]
    }
   ],
   "source": [
    "# train test split into training and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(pred_dfs, gt_dfs, \n",
    "                            test_size=config.val_split, random_state=2)\n",
    "\n",
    "\n",
    "train_dataset = GNSSDataset(X_train, y_train)\n",
    "val_dataset = GNSSDataset(X_val, y_val)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100 # number of epochs\n",
    "n_eval = 10 # evaluate every n_eval epochs\n",
    "lr = 0.0001 # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d727106929a44e0fa80fa7a778824e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/train 309.1878356933594\n",
      "torch.Size([8, 3500, 2])\n",
      "Val data 0\n",
      "Pred: (37.3089371  , -121.9904861) Ground Truth: (37.2889252  , -121.9904709)\n",
      "Pred: (37.3089333  , -121.9905472) Ground Truth: (37.2889252  , -121.9905243)\n",
      "Pred: (37.3089294  , -121.9906845) Ground Truth: (37.2889290  , -121.9905777)\n",
      "Pred: (37.3089027  , -121.9906921) Ground Truth: (37.2889328  , -121.9906387)\n",
      "Pred: (37.3089371  , -121.9907074) Ground Truth: (37.2889404  , -121.9906998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_349/2214136882.py:61: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  lat_1 = np.deg2rad(blh_1.lat)\n",
      "/tmp/ipykernel_349/2214136882.py:62: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  lng_1 = np.deg2rad(blh_1.lng)\n",
      "/tmp/ipykernel_349/2214136882.py:63: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  lat_2 = np.deg2rad(blh_2.lat)\n",
      "/tmp/ipykernel_349/2214136882.py:64: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  lng_2 = np.deg2rad(blh_2.lng)\n",
      "/tmp/ipykernel_349/2214136882.py:68: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  a = np.sin(dlat/2)**2 + np.cos(lat_1) * np.cos(lat_2) * np.sin(dlng/2)**2\n",
      "/tmp/ipykernel_349/2214136882.py:69: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  dist = 2 * HAVERSINE_RADIUS * np.arcsin(np.sqrt(a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val mean dist 13901.3984375\n",
      "Val mean score 11018.6513671875\n",
      "Loss/val 0.18737797439098358\n"
     ]
    }
   ],
   "source": [
    "# Initalize optimizer (for gradient descent) and loss function\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = lr)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "step = 0\n",
    "\n",
    "PATH = \"model.pt\"\n",
    "best_loss = 99999999 #high number\n",
    "\n",
    "# Regularization strength\n",
    "lambda_l1 = 0.1\n",
    "lambda_l2 = 0.25\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "\n",
    "    # Loop over each batch in the dataset\n",
    "    for batch in tqdm(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad() # If not, the gradients would sum up over each iteration\n",
    "\n",
    "        # Unpack the data and labels\n",
    "        features, labels = batch\n",
    "        \n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward propagate\n",
    "\n",
    "        outputs = model(features)\n",
    "\n",
    "        # Backpropagation and gradient descent            \n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # L1 regularization\n",
    "        l1_reg = sum(param.abs().sum() for param in model.parameters())\n",
    "        # L2 regularization\n",
    "        l2_reg = sum(param.pow(2).sum() for param in model.parameters())\n",
    "\n",
    "        # Add the regularization terms to the loss\n",
    "        loss += lambda_l1 * l1_reg + lambda_l2 * l2_reg\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'Loss/train {loss}')\n",
    "\n",
    "        # Periodically evaluate our model + log to Tensorboard\n",
    "        if step % n_eval == 0:\n",
    "            # Compute training loss and metrics\n",
    "            \n",
    "            model.eval()\n",
    "            mean_dist = 0\n",
    "            mean_score = 0\n",
    "            count = 0\n",
    "            val_loss = 0\n",
    "            for features, labels in val_loader:\n",
    "#                 print(features.shape)\n",
    "#                 print(features[0, :20])\n",
    "                features = features.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                val_pred = model(features)\n",
    "                \n",
    "                val_loss = loss_fn(val_pred, labels)\n",
    "                \n",
    "                features = features.detach().cpu()# * 180\n",
    "                val_pred = val_pred.detach().cpu()# * 180\n",
    "                labels = labels.detach().cpu()# * 180\n",
    "                print(val_pred.shape)\n",
    "                pred_lats = val_pred[:, :, 0] + features[:, :, 0]\n",
    "                pred_lngs = val_pred[:, :, 1] + features[:, :, 1]\n",
    "                gt_lats = labels[:, :, 0] + features[:, :, 0]\n",
    "                gt_lns = labels[:, :, 1] + features[:, :, 1]\n",
    "                \n",
    "                start = 60\n",
    "                cutoff = 65\n",
    "                # print some samples\n",
    "                print_batch(1, pred_lats[:, start:cutoff], pred_lngs[:, start:cutoff], gt_lats[:, start:cutoff], gt_lns[:, start:cutoff]) \n",
    "                \n",
    "                # Calculate score according to kaggle, height not necessary for distance\n",
    "                blh1 = BLH(pred_lats, pred_lngs, hgt = 0)\n",
    "                blh2 = BLH(gt_lats, gt_lns, hgt = 0)\n",
    "                d = haversine_distance(blh1, blh2)\n",
    "                mean_score += np.mean([np.quantile(d, 0.50), np.quantile(d, 0.95)])     \n",
    "                mean_dist += d.mean()  \n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "                features = features.cpu()\n",
    "                labels = labels.cpu()\n",
    "                \n",
    "            if(val_loss < best_loss):\n",
    "                best_loss = val_loss\n",
    "                \n",
    "                torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model.state_dict(),\n",
    "                            'optimizer_state_dict': optimizer.state_dict(),\n",
    "                            'loss': val_loss,\n",
    "                            }, PATH)\n",
    "\n",
    "                \n",
    "            print(f'Val mean dist {mean_dist}')\n",
    "            print(f'Val mean score {mean_score}')\n",
    "            print(f'Loss/val {val_loss}')\n",
    "\n",
    "            #turn on training, evaluate turns off training\n",
    "            model.train()\n",
    "\n",
    "        step += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
